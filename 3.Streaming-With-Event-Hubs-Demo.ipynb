{"cells":[{"cell_type":"markdown","source":["# Structured Streaming with Azure EventHubs \n\n## Datasets Used\nThis notebook will consumn data being published through an EventHub with the following schema:\n\n- `Index`\n- `Arrival_Time`\n- `Creation_Time`\n- `x`\n- `y`\n- `z`\n- `User`\n- `Model`\n- `Device`\n- `gt`\n- `id`\n- `geolocation`\n\n## Library Requirements\n\n1. the Maven library with coordinate `com.microsoft.azure:azure-eventhubs-spark_2.12:2.3.18`\n   - this allows Databricks `spark` session to communicate with an Event Hub\n2. the Python library `azure-eventhub`\n   - this is allows the Python kernel to stream content to an Event Hub\n\nThe next cell walks you through installing the Maven library. A couple cells below that, we automatically install the Python library using `%pip install`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ecf745db-3d5a-4918-8d07-f3a5d3ccdbf5"}}},{"cell_type":"markdown","source":["## Lab Setup\n\nIf you are running in an Azure Databricks environment that is already pre-configured with the libraries you need, you can skip to the next cell. To use this notebook in your own Databricks environment, you will need to create libraries, using the [Create Library](https://docs.azuredatabricks.net/user-guide/libraries.html) interface in Azure Databricks. Follow the steps below to attach the `azure-eventhubs-spark` library to your cluster:\n\n1. In the left-hand navigation menu of your Databricks workspace, select **Compute**, then select your cluster in the list. If it's not running, start it now.\n\n  ![Select cluster](https://databricksdemostore.blob.core.windows.net/images/10-de-learning-path/select-cluster.png)\n\n2. Select the **Libraries** tab (1), then select **Install New** (2). In the Install Library dialog, select **Maven** under Library Source (3). Under Coordinates, paste **com.microsoft.azure:azure-eventhubs-spark_2.12:2.3.18** (4), then select **Install**.\n  \n  ![Databricks new Maven library](https://databricksdemostore.blob.core.windows.net/images/10-de-learning-path/install-eventhubs-spark-library.png)\n\n3. Wait until the library successfully installs before continuing.\n\n  ![Library installed](https://databricksdemostore.blob.core.windows.net/images/10-de-learning-path/eventhubs-spark-library-installed.png)\n\nOnce complete, return to this notebook to continue with the lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3daddb4-ab6a-4229-9529-c3f0d6061009"}}},{"cell_type":"markdown","source":["### Getting Started\n\nRun the following two cells to install the `azure-eventhub` Python library and configure our \"classroom.\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"221bffa2-02b4-4cd0-84ae-721a3b895a94"}}},{"cell_type":"code","source":["# This library allows the Python kernel to stream content to an Event Hub:\n%pip install azure-eventhub"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8150325f-906b-45b3-bd3b-02e24cec852a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf492a64-6282-4987-be4f-cb1dfc5d4577"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["The following cell sets up a local streaming file read that we'll be writing to Event Hubs."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"08b5a1f9-1090-4313-977d-1855881e503e"}}},{"cell_type":"code","source":["%run ./Includes/Streaming-Demo-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa5948ac-eb66-49e8-acff-1fbdf04a83aa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["In order to reach Event Hubs, you will need to insert the connection string-primary key you acquired at the end of the Getting Started notebook in this module. You acquired this from the Azure Portal, and copied it into Notepad.exe or another text editor.\n\n> Read this article to learn [how to acquire the connection string for an Event Hub](https://docs.microsoft.com/azure/event-hubs/event-hubs-create) in your own Azure Subscription."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7a7abade-e10a-430c-9f1e-09c1b1c0349f"}}},{"cell_type":"code","source":["event_hub_connection_string = \"REPLACE-WITH-YOUR-EVENT-HUBS-CONNECTION-STRING\" # Paste your Event Hubs connection string in the quotes to the left"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"85c323e0-5272-4216-bc51-80e980065c31"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Azure Event Hubs</h2>\n\nMicrosoft Azure Event Hubs is a fully managed, real-time data ingestion service.\nYou can stream millions of events per second from any source to build dynamic data pipelines and immediately respond to business challenges.\nIt integrates seamlessly with a host of other Azure services.\n\nEvent Hubs can be used in a variety of applications such as\n* Anomaly detection (fraud/outliers)\n* Application logging\n* Analytics pipelines, such as clickstreams\n* Archiving data\n* Transaction processing\n* User telemetry processing\n* Device telemetry streaming\n* <b>Live dashboarding</b>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c0760e1-9bad-49ca-aefd-10a4dbe72845"}}},{"cell_type":"code","source":["%python\n\nevent_hub_name = \"databricks-demo-eventhub\"\nconnection_string = event_hub_connection_string + \";EntityPath=\" + event_hub_name\n\nprint(\"Consumer Connection String: {}\".format(connection_string))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"093c02f4-e628-4977-9cf0-f689aa329cb4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### Write Stream to Event Hub to Produce Stream"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e671cbb-ecaf-4d1f-83e7-6bfeb75ff234"}}},{"cell_type":"code","source":["%python\n\n# For 2.3.15 version and above, the configuration dictionary requires that connection string be encrypted.\nehWriteConf = {\n  'eventhubs.connectionString' : sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connection_string)\n}\n\ncheckpointPath = userhome + \"/event-hub/write-checkpoint\"\ndbutils.fs.rm(checkpointPath,True)\n\n(activityStreamDF\n  .writeStream\n  .format(\"eventhubs\")\n  .options(**ehWriteConf)\n  .option(\"checkpointLocation\", checkpointPath)\n  .start())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6ffe932b-c2bb-45b1-a187-990fb8145840"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Event Hubs Configuration</h2>\n\nAssemble the following:\n* A `startingEventPosition` as a JSON string\n* An `EventHubsConf`\n  * to include a string with connection credentials\n  * to set a starting position for the stream read\n  * to throttle Event Hubs' processing of the streams"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c02e996-b843-4673-9f6e-60e83ee8ca21"}}},{"cell_type":"code","source":["%python\n\nimport json\n\n# Create the starting position Dictionary\nstartingEventPosition = {\n  \"offset\": \"-1\",\n  \"seqNo\": -1,            # not in use\n  \"enqueuedTime\": None,   # not in use\n  \"isInclusive\": True\n}\n\neventHubsConf = {\n  \"eventhubs.connectionString\" : sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connection_string),\n  \"eventhubs.startingPosition\" : json.dumps(startingEventPosition),\n  \"setMaxEventsPerTrigger\": 100\n}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11c4cda5-e187-4f83-b259-90f33f317b6e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### READ Stream using Event Hubs\n\nThe `readStream` method is a <b>transformation</b> that outputs a DataFrame with specific schema specified by `.schema()`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"306ebc06-09ec-42bf-a0cd-83b52a8ac9e2"}}},{"cell_type":"code","source":["%python\n\nfrom pyspark.sql.functions import col\n\nspark.conf.set(\"spark.sql.shuffle.partitions\", sc.defaultParallelism)\n\neventStreamDF = (spark.readStream\n  .format(\"eventhubs\")\n  .options(**eventHubsConf)\n  .load()\n)\n\neventStreamDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"62095865-6099-417d-8263-1941cbbda4c9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Most of the fields in this response are metadata describing the state of the Event Hubs stream. We are specifically interested in the `body` field, which contains our JSON payload.\n\nNoting that it's encoded as binary, as we select it, we'll cast it to a string."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"57d7a5ea-44b1-4e32-aa1c-8615ffc25d6e"}}},{"cell_type":"code","source":["%python\nbodyDF = eventStreamDF.select(col(\"body\").cast(\"STRING\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"868d0984-9d06-4897-8f52-044eca4ae49f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Each line of the streaming data becomes a row in the DataFrame once an <b>action</b> such as `writeStream` is invoked.\n\nNotice that nothing happens until you engage an action, i.e. a `display()` or `writeStream`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"907be8a8-4ba6-4e88-b6e0-2b4377c60f09"}}},{"cell_type":"code","source":["%python\ndisplay(bodyDF, streamName= \"bodyDF\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ee8cdb1a-9d62-4ba3-9722-c89dec5e6321"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["While we can see our JSON data now that it's cast to string type, we can't directly manipulate it.\n\nBefore proceeding, stop this stream. We'll continue building up transformations against this streaming DataFrame, and a new action will trigger an additional stream."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1af09ffa-a04d-4d37-b8d3-2348c781e09f"}}},{"cell_type":"code","source":["%python\nfor s in spark.streams.active:\n  if s.name == \"bodyDF\":\n    s.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"44a1b348-2e8d-4aab-b3f0-d140a0fc8813"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## <img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Parse the JSON payload\n\nThe Event Hub acts as a sort of \"firehose\" (or asynchronous buffer) and displays raw data in the JSON format.\n\nIf desired, we could save this as raw bytes or strings and parse these records further downstream in our processing.\n\nHere, we'll directly parse our data so we can interact with the fields.\n\nThe first step is to define the schema for the JSON payload.\n\n> Both time fields are encoded as `LongType` here because of non-standard formatting."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e6a7129-35de-4a2e-9c09-b24664dd2346"}}},{"cell_type":"code","source":["%python\n\nfrom pyspark.sql.types import StructField, StructType, StringType, LongType, DoubleType\n\nschema = StructType([\n  StructField(\"Arrival_Time\", LongType(), True),\n  StructField(\"Creation_Time\", LongType(), True),\n  StructField(\"Device\", StringType(), True),\n  StructField(\"Index\", LongType(), True),\n  StructField(\"Model\", StringType(), True),\n  StructField(\"User\", StringType(), True),\n  StructField(\"gt\", StringType(), True),\n  StructField(\"x\", DoubleType(), True),\n  StructField(\"y\", DoubleType(), True),\n  StructField(\"z\", DoubleType(), True),\n  StructField(\"geolocation\", StructType([\n    StructField(\"PostalCode\", StringType(), True),\n    StructField(\"StateProvince\", StringType(), True),\n    StructField(\"city\", StringType(), True),\n    StructField(\"country\", StringType(), True)\n  ]), True),\n  StructField(\"id\", StringType(), True)\n])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d7e0476b-b6b2-4476-ac3d-7ebf78e1a2bd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### Parse the data\n\nNext we can use the function `from_json` to parse out the full message with the schema specified above.\n\nWhen parsing a value from JSON, we end up with a single column containing a complex object."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0eac6d5e-aecc-48a2-87b0-2b86185b2c45"}}},{"cell_type":"code","source":["%python\n\nfrom pyspark.sql.functions import col, from_json\n\nparsedEventsDF = bodyDF.select(\n  from_json(col(\"body\"), schema).alias(\"json\"))\n\nparsedEventsDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3db04f92-9469-422f-9c61-4b0349f7eb5e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Note that we can further parse this to flatten the schema entirely and properly cast our time fields."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"02870873-f182-4316-821b-3e51306aeeec"}}},{"cell_type":"code","source":["%python\n\nfrom pyspark.sql.functions import from_unixtime\n\nflatSchemaDF = (parsedEventsDF\n  .select(from_unixtime(col(\"json.Arrival_Time\")/1000).alias(\"Arrival_Time\").cast(\"timestamp\"),\n          (col(\"json.Creation_Time\")/1E9).alias(\"Creation_Time\").cast(\"timestamp\"),\n          col(\"json.Device\").alias(\"Device\"),\n          col(\"json.Index\").alias(\"Index\"),\n          col(\"json.Model\").alias(\"Model\"),\n          col(\"json.User\").alias(\"User\"),\n          col(\"json.gt\").alias(\"gt\"),\n          col(\"json.x\").alias(\"x\"),\n          col(\"json.y\").alias(\"y\"),\n          col(\"json.z\").alias(\"z\"),\n          col(\"json.id\").alias(\"id\"),\n          col(\"json.geolocation.country\").alias(\"country\"),\n          col(\"json.geolocation.city\").alias(\"city\"),\n          col(\"json.geolocation.PostalCode\").alias(\"PostalCode\"),\n          col(\"json.geolocation.StateProvince\").alias(\"StateProvince\"))\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e1bb285-3d6e-4d67-b2b7-98af9ce2b4b1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["This flat schema provides us the ability to view each nested field as a column."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aeff8317-7136-4082-8cca-9c84309beb6b"}}},{"cell_type":"code","source":["%python\ndisplay(flatSchemaDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ba4e84e2-549d-46e8-9b92-bdf393bc9021"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### Stop all active streams"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"207c16fc-0185-4cfc-8a0f-ecde383574ca"}}},{"cell_type":"code","source":["%python\nfor s in spark.streams.active:\n  s.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b4b1df07-a435-45b8-911b-d500f64bba37"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["#### Event Hubs FAQ\n\nThis [FAQ](https://github.com/Azure/azure-event-hubs-spark/blob/master/FAQ.md) can be an invaluable reference for occasional Spark-EventHub debugging."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"01ba235e-0392-4425-b7e4-15c20047dd56"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"3.Streaming-With-Event-Hubs-Demo","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1974493220638322}},"nbformat":4,"nbformat_minor":0}

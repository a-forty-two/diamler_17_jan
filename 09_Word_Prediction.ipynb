{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "09 Word Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNtWkrtPOPVlJCcePHzaGmH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a-forty-two/diamler_17_jan/blob/main/09_Word_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "147TqC9T0cg5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a4fc43b6-3044-4392-d543-7542f4cc8e51"
      },
      "source": [
        "# BODY -> body of string from which we will dervice our vectors\n",
        "corpus_raw = 'Richie Rich is rich. The rich person is a happy person. He and She are not rich.'\n",
        "corpus_lower = corpus_raw.lower()\n",
        "corpus_lower"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'richie rich is rich. the rich person is a happy person. he and she are not rich.'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB1pYErB05gb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4710604-8459-4413-ce35-7d2bc954e7cc"
      },
      "source": [
        "# Segmentation-> break into sentences\n",
        "# Tokenization-> getting every word separately\n",
        "words = []\n",
        "for word in corpus_lower.split():\n",
        "  if word == '.':\n",
        "    # if punctuation is not individual word but a part of word-> bye.\n",
        "    word = word.replace('.', '') # every punctuation that we don't want should be filtered here\n",
        "    words.append(word)\n",
        "  else:\n",
        "    words.append(word)\n",
        "\n",
        "print(words)\n",
        "# TO CONVERT TO VECTORS , we don't want repeating! \n",
        "# Which DS allows us to avoid repeating? SET!!\n",
        "print(len(words))\n",
        "unique_words = set(words)\n",
        "print(unique_words)\n",
        "print(len(unique_words))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['richie', 'rich', 'is', 'rich.', 'the', 'rich', 'person', 'is', 'a', 'happy', 'person.', 'he', 'and', 'she', 'are', 'not', 'rich.']\n",
            "17\n",
            "{'rich', 'the', 'is', 'not', 'and', 'are', 'person', 'happy', 'he', 'person.', 'rich.', 'she', 'richie', 'a'}\n",
            "14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjOiG1jl1i1m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d383ec2-5c7c-41c4-e295-f5bbf7f1d5bb"
      },
      "source": [
        "word2int = {} \n",
        "int2word = {}\n",
        "vocab_size = len(unique_words)\n",
        "\n",
        "# enumerate when passed a list, iterates on that list in index,value format!\n",
        "# a-> [a,b,c]\n",
        "# enumerate(a) -> (0,a), (1,b), (2,c)\n",
        "for i,word in enumerate(unique_words):\n",
        "  word2int[word] =i \n",
        "  int2word[i] = word\n",
        "\n",
        "print(word2int) # Encoder Dict\n",
        "print(int2word) # Decoder Dict\n",
        "print(word2int['rich'])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rich': 0, 'the': 1, 'is': 2, 'not': 3, 'and': 4, 'are': 5, 'person': 6, 'happy': 7, 'he': 8, 'person.': 9, 'rich.': 10, 'she': 11, 'richie': 12, 'a': 13}\n",
            "{0: 'rich', 1: 'the', 2: 'is', 3: 'not', 4: 'and', 5: 'are', 6: 'person', 7: 'happy', 8: 'he', 9: 'person.', 10: 'rich.', 11: 'she', 12: 'richie', 13: 'a'}\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKxVFQtT23IR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae442d80-20cd-4870-9e63-79b28f4d8dec"
      },
      "source": [
        "# SENTENCE level tokens so that we know what a complete sample looks like!\n",
        "# please note: i have only 3 sentences, so dicitonary, prediction everything is going\n",
        "# to be very poor!\n",
        "\n",
        "raw_sentences = corpus_raw.split('.') # now breaking sentences instead of ' '\n",
        "sentences = []\n",
        "for sentence in raw_sentences:\n",
        "  sentences.append(sentence.split()) # SPLIT -> BOTH SENTENCE and WORD tokens are created in 1go!\n",
        "\n",
        "print(sentences) # contains extra [] because of the LAST '.' \n",
        "# sentences split the last full stop also became and element! \n",
        "sentences = sentences[:-1]\n",
        "sentences"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Richie', 'Rich', 'is', 'rich'], ['The', 'rich', 'person', 'is', 'a', 'happy', 'person'], ['He', 'and', 'She', 'are', 'not', 'rich'], []]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Richie', 'Rich', 'is', 'rich'],\n",
              " ['The', 'rich', 'person', 'is', 'a', 'happy', 'person'],\n",
              " ['He', 'and', 'She', 'are', 'not', 'rich']]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw3zlGd641O7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4203dfba-62a5-4693-fd97-bb33590c3e7d"
      },
      "source": [
        "# i wanted to create word prediction\n",
        "# ML algo?\n",
        "# y-> next word, x -> previous word\n",
        "# y = weights * x + bias \n",
        "# next_word = weights * previous_word + bias \n",
        "\n",
        "# BASICALLY what i need is ALL possible pairs of WORDs that can follow each other!!!\n",
        "\n",
        "window_size = 2 # 2 words at a time\n",
        "data = [] \n",
        "for sentence in sentences: # loop in sentence\n",
        "  for wordId, word in enumerate(sentence): # loop in word\n",
        "    for nxword in sentence[max(wordId-window_size,0):min(wordId+window_size,len(sentence))+1]:  \n",
        "      if nxword!=word:\n",
        "        data.append([word,nxword])\n",
        "         # every word that could come next to it! \n",
        "      # no. of elements = len-window_size \n",
        "      # max -> 0, 1 at that point 0-2 and 1-2 will give me neg nos. hence error\n",
        "      # to fix it we used max with 0! \n",
        "      # min -> at the end of array-> to avoid crossing the max len of array! \n",
        "      # +1 because the last element is excluded in numpy ranges [1,2]-> from 1 till 1!\n",
        "\n",
        "print(data)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Richie', 'Rich'], ['Richie', 'is'], ['Rich', 'Richie'], ['Rich', 'is'], ['Rich', 'rich'], ['is', 'Richie'], ['is', 'Rich'], ['is', 'rich'], ['rich', 'Rich'], ['rich', 'is'], ['The', 'rich'], ['The', 'person'], ['rich', 'The'], ['rich', 'person'], ['rich', 'is'], ['person', 'The'], ['person', 'rich'], ['person', 'is'], ['person', 'a'], ['is', 'rich'], ['is', 'person'], ['is', 'a'], ['is', 'happy'], ['a', 'person'], ['a', 'is'], ['a', 'happy'], ['a', 'person'], ['happy', 'is'], ['happy', 'a'], ['happy', 'person'], ['person', 'a'], ['person', 'happy'], ['He', 'and'], ['He', 'She'], ['and', 'He'], ['and', 'She'], ['and', 'are'], ['She', 'He'], ['She', 'and'], ['She', 'are'], ['She', 'not'], ['are', 'and'], ['are', 'She'], ['are', 'not'], ['are', 'rich'], ['not', 'She'], ['not', 'are'], ['not', 'rich'], ['rich', 'are'], ['rich', 'not']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9Mi_3zS7E1d"
      },
      "source": [
        "# Label Encoding: CONVERTING words to numbers is called LABEL encoding \n",
        "# [hello, word] -> [1,2 ]\n",
        "\n",
        "# One-Hot Encoding: we converted categorical values into columns containing 1 and 0 only\n",
        "# Sales_ City_             Sales_  City_BLR City_Mum \n",
        "#  100    BLR    ->         100       1       0\n",
        "#  200    MUM               200       0       1\n",
        "\n",
        "# Multi-Hot Encoding : A sentence is matched against a dictionary, and existing words are \n",
        "# marked hot (1), absent words are marked cold (0)\n",
        "# Dictionary : { good, boy, am, I, not, green} \n",
        "# Sentence: green not good \n",
        "# Multi_hot_encoding:   1 0 0 0 1 1\n",
        "# in multi hot encoding, words lose their positional meaning! \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alaOv5nh-Ha3"
      },
      "source": [
        "import numpy as np\n",
        "def Encoder(datapointindex, vocabsize):\n",
        "  temp = np.zeros(vocabsize) # this is an ALL-COLD VECTOR! \n",
        "  temp[datapointindex] = 1 # wherever word exists, datapointindex is it's position\n",
        "  return temp\n",
        "\n",
        "trainx = []\n",
        "trainy = []"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K15Uvl1F_KJK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50f9092d-0631-4722-e0ad-1785e83028cc"
      },
      "source": [
        "for worddata in data: # worddata[0]-> prev word (train-input), [1]-> next word (train-label!)\n",
        "  try:\n",
        "    trainx.append(Encoder(word2int[worddata[0]], vocab_size))\n",
        "    trainy.append(Encoder(word2int[worddata[1]], vocab_size))\n",
        "  except:\n",
        "    continue\n",
        "\n",
        "\n",
        "# adjusting just for this demo\n",
        "trainx = trainx[:len(trainy)]\n",
        "trainx = np.asarray(trainx)\n",
        "trainy = np.asarray(trainy)\n",
        "print(trainx.shape)\n",
        "print(trainy.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28, 14)\n",
            "(28, 14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6lKzU3VCgXR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDS1Q-B3_xM1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95533fd3-ae5c-4f93-9474-d9ea240fabb9"
      },
      "source": [
        "trainx[:5]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMU7_YkS_7yq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "749830a0-f7f9-43b0-abc9-28277813a57d"
      },
      "source": [
        "trainy[:5]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6koDjk_Bl2t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}